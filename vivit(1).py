# -*- coding: utf-8 -*-
"""vivit(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hy04b-LChNGWs2LFP5Q66Ap6M5ah-pSt
"""

#Install Required Packages
!pip install transformers[torch]
!pip install datasets
!pip install torch torchvision torchaudio
!pip install opencv-python
!pip install scikit-learn
!pip install matplotlib seaborn
!pip install tqdm
!pip install kaggle
!pip install accelerate
!pip install av

#Import Libraries
import os
import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import VivitImageProcessor, VivitForVideoClassification, VivitConfig
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json
import random
from pathlib import Path
import warnings
import kagglehub
warnings.filterwarnings('ignore')

#Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Download the dataset
data_path = kagglehub.dataset_download("xdxd003/ff-c23")

dataset_path = os.path.join(data_path, "FaceForensics++_C23")
real_videos_path = os.path.join(dataset_path, "original")
fake_video_folders = [
    "DeepFakeDetection",
    "Deepfakes",
    "Face2Face",
    "FaceShifter",
    "FaceSwap",
    "NeuralTextures"
]

# Check if paths exist
print(f"Real videos path exists: {os.path.exists(real_videos_path)}")
for folder in fake_video_folders:
    folder_path = os.path.join(dataset_path, folder)
    print(f"{folder} path exists: {os.path.exists(folder_path)}")

def get_video_files(directory):
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']
    video_files = []

    for ext in video_extensions:
        video_files.extend(Path(directory).glob(f"**/*{ext}"))

    return [str(f) for f in video_files]

# Updated data preparation section
print("Collecting video files...")

# Get real videos
real_videos = get_video_files(real_videos_path)
print(f"Found {len(real_videos)} real videos")

# Get fake videos from all fake folders
fake_videos = []
for folder in fake_video_folders:
    folder_path = os.path.join(dataset_path, folder)
    if os.path.exists(folder_path):
        folder_videos = get_video_files(folder_path)
        fake_videos.extend(folder_videos)
        print(f"Found {len(folder_videos)} videos in {folder}")

print(f"Total fake videos: {len(fake_videos)}")

# Create labels (0 for real, 1 for fake)
real_labels = [0] * len(real_videos)
fake_labels = [1] * len(fake_videos)

# Combine data
all_videos = real_videos + fake_videos
all_labels = real_labels + fake_labels

print(f"Total videos: {len(all_videos)}")
print(f"Real videos: {len(real_videos)} ({len(real_videos)/len(all_videos)*100:.1f}%)")
print(f"Fake videos: {len(fake_videos)} ({len(fake_videos)/len(all_videos)*100:.1f}%)")

# Add class balancing
def balance_dataset(real_videos, fake_videos, max_samples_per_class=500):
    # Sample up to max_samples_per_class for each class
    sampled_real = random.sample(real_videos, min(len(real_videos), max_samples_per_class))
    sampled_fake = random.sample(fake_videos, min(len(fake_videos), max_samples_per_class))

    # Create labels (0=real, 1=fake)
    real_labels = [0] * len(sampled_real)
    fake_labels = [1] * len(sampled_fake)

    # Combine and shuffle
    balanced_videos = sampled_real + sampled_fake
    balanced_labels = real_labels + fake_labels

    # Shuffle while maintaining correspondence
    combined = list(zip(balanced_videos, balanced_labels))
    random.shuffle(combined)
    balanced_videos, balanced_labels = zip(*combined)

    return list(balanced_videos), list(balanced_labels)

# Apply balancing if dataset is heavily imbalanced
if len(real_videos) != len(fake_videos):
    print("Balancing dataset...")
    all_videos, all_labels = balance_dataset(all_videos, all_labels, max_samples_per_class=500)
    print(f"Balanced dataset size: {len(all_videos)} videos")

# Video Processing
def extract_frames_from_video(video_path, num_frames=32, target_size=(224, 224)):

    cap = cv2.VideoCapture(video_path)
    frames = []

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if total_frames < num_frames:
        # If video has fewer frames than needed, duplicate frames
        frame_indices = []
        for i in range(num_frames):
            frame_indices.append(min(i, total_frames - 1))
    else:
        # Sample frames uniformly
        frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)

    for frame_idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()

        if ret:
            # Convert BGR to RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # Resize frame to exact target size
            frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_LINEAR)
            frames.append(frame)
        else:
            # Create black frame if reading fails
            frames.append(np.zeros((*target_size, 3), dtype=np.uint8))

    cap.release()

    # Ensure we have exactly num_frames
    frames = frames[:num_frames]  # Truncate if too many
    while len(frames) < num_frames:
        if frames:
            frames.append(frames[-1])  # Duplicate last frame
        else:
            frames.append(np.zeros((*target_size, 3), dtype=np.uint8))

    return np.array(frames)

def get_video_files(directory):
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']
    video_files = []

    for ext in video_extensions:
        video_files.extend(Path(directory).glob(f"**/*{ext}"))

    return [str(f) for f in video_files]

# Create Dataset Class
class DeepFakeDataset(Dataset):
    def __init__(self, video_paths, labels, processor, num_frames=32):
        self.video_paths = video_paths
        self.labels = labels
        self.processor = processor
        self.num_frames = num_frames

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        label = self.labels[idx]

        try:
            # Extract frames from video
            frames = extract_frames_from_video(video_path, self.num_frames, target_size=(224, 224))

            # Convert to list for processor
            frames_list = [frames[i] for i in range(len(frames))]

            # Process frames using ViViT processor
            inputs = self.processor(frames_list, return_tensors="pt", do_resize=False)

            # Get pixel values and ensure correct shape
            pixel_values = inputs['pixel_values'].squeeze(0)  # Remove batch dimension

            return {
                'pixel_values': pixel_values,
                'labels': torch.tensor(label, dtype=torch.long)
            }
        except Exception as e:
            print(f"Error processing video {video_path}: {str(e)}")
            # Return dummy data in case of error
            dummy_frames = [np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(self.num_frames)]
            inputs = self.processor(dummy_frames, return_tensors="pt", do_resize=False)
            pixel_values = inputs['pixel_values'].squeeze(0)

            return {
                'pixel_values': pixel_values,
                'labels': torch.tensor(label, dtype=torch.long)
            }

            return {
                'pixel_values': pixel_values,
                'labels': torch.tensor(label, dtype=torch.long)
            }

# Prepare Data
print(f"Found {len(real_videos)} real videos")
print(f"Found {len(fake_videos)} fake videos")

print(f"\nBalanced dataset statistics:")
print(f"Total videos: {len(all_videos)}")
print(f"Real videos: {all_labels.count(0)}")
print(f"Fake videos: {all_labels.count(1)}")

# Split data into train/val/test with stratification
def split_data(videos, labels, val_size=0.1, test_size=0.1, random_seed=42):
    # First split into train+val and test
    train_val_videos, test_videos, train_val_labels, test_labels = train_test_split(
        videos, labels,
        test_size=test_size,
        random_state=random_seed,
        stratify=labels
    )

    # Then split train+val into train and val
    train_videos, val_videos, train_labels, val_labels = train_test_split(
        train_val_videos, train_val_labels,
        test_size=val_size/(1-test_size),  # Adjust for initial split
        random_state=random_seed,
        stratify=train_val_labels
    )

    return train_videos, val_videos, test_videos, train_labels, val_labels, test_labels

# Split the data (60% train, 20% val, 20% test)
train_videos, val_videos, test_videos, train_labels, val_labels, test_labels = split_data(
    all_videos, all_labels, val_size=0.2, test_size=0.2
)

print("\nFinal dataset splits:")
print(f"Train: {len(train_videos)} videos ({100*sum(train_labels)/len(train_labels):.1f}% fake)")
print(f"Validation: {len(val_videos)} videos ({100*sum(val_labels)/len(val_labels):.1f}% fake)")
print(f"Test: {len(test_videos)} videos ({100*sum(test_labels)/len(test_labels):.1f}% fake)")

# Initialize ViViT Model and Processor
model_name = "google/vivit-b-16x2-kinetics400"
processor = VivitImageProcessor.from_pretrained(model_name)

print(f"Processor config:")
print(f"- Size: {processor.size}")
print(f"- Do normalize: {processor.do_normalize}")
print(f"- Do resize: {processor.do_resize}")

# Create custom config for our binary classification task
config = VivitConfig.from_pretrained(model_name)
config.num_labels = 2  # Binary classification: real vs fake

print(f"Model config:")
print(f"- Num labels: {config.num_labels}")
print(f"- Num frames: {config.num_frames}")
print(f"- Image size: {config.image_size}")
print(f"- Hidden size: {config.hidden_size}")

# Load pre-trained model with custom config
model = VivitForVideoClassification.from_pretrained(
    model_name,
    config=config,
    ignore_mismatched_sizes=True  # Allow classifier head to be different
)

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

print(f"Model loaded and moved to {device}")
print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

# Print model architecture summary
print("\nModel architecture:")
for name, module in model.named_children():
    print(f"- {name}: {type(module).__name__}")

print(f"\nClassifier details:")
print(f"- Input features: {model.classifier.in_features}")
print(f"- Output features: {model.classifier.out_features}")

# Create Data Loaders

# Create datasets
train_dataset = DeepFakeDataset(train_videos, train_labels, processor, num_frames=32)
val_dataset = DeepFakeDataset(val_videos, val_labels, processor, num_frames=32)
test_dataset = DeepFakeDataset(test_videos, test_labels, processor, num_frames=32)

# Create data loaders
batch_size = 2  # Reduced batch size for memory efficiency with 32 frames
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

print(f"Data loaders created with batch size: {batch_size}")
print(f"Train batches: {len(train_loader)}")
print(f"Validation batches: {len(val_loader)}")
print(f"Test batches: {len(test_loader)}")

# Test data loading
print("\nTesting data loading...")
try:
    sample_batch = next(iter(train_loader))
    pixel_values = sample_batch['pixel_values']
    labels = sample_batch['labels']

    print(f"Sample batch shape:")
    print(f"- Pixel values: {pixel_values.shape}")
    print(f"- Labels: {labels.shape}")

    # Test forward pass
    pixel_values = pixel_values.to(device)
    labels = labels.to(device)

    print(f"\nTesting forward pass...")
    with torch.no_grad():
        outputs = model(pixel_values)
        print(f"- Output logits shape: {outputs.logits.shape}")
        print("Forward pass successful!")

except Exception as e:
    print(f"Error in data loading test: {str(e)}")
    print("Please check the video processing and model configuration.")

# Training parameters
num_epochs = 5
learning_rate = 2e-5
weight_decay = 0.01

# Optimizer and loss function
optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
criterion = nn.CrossEntropyLoss()

# Learning rate scheduler
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

# Training history
train_losses = []
val_losses = []
train_accuracies = []
val_accuracies = []

print("Training setup complete!")

# Training Function
def train_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(train_loader, desc="Training")

    for batch in progress_bar:
        # Move data to device
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)

        # Zero gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(pixel_values)
        loss = criterion(outputs.logits, labels)

        # Backward pass
        loss.backward()
        optimizer.step()

        # Statistics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.logits.data, 1)
        total_predictions += labels.size(0)
        correct_predictions += (predicted == labels).sum().item()

        # Update progress bar
        progress_bar.set_postfix({
            'Loss': f'{running_loss/len(progress_bar):.4f}',
            'Acc': f'{100*correct_predictions/total_predictions:.2f}%'
        })

    epoch_loss = running_loss / len(train_loader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc

def validate_epoch(model, val_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    with torch.no_grad():
        progress_bar = tqdm(val_loader, desc="Validation")

        for batch in progress_bar:
            # Move data to device
            pixel_values = batch['pixel_values'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(pixel_values)
            loss = criterion(outputs.logits, labels)

            # Statistics
            running_loss += loss.item()
            _, predicted = torch.max(outputs.logits.data, 1)
            total_predictions += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()

            # Update progress bar
            progress_bar.set_postfix({
                'Loss': f'{running_loss/len(progress_bar):.4f}',
                'Acc': f'{100*correct_predictions/total_predictions:.2f}%'
            })

    epoch_loss = running_loss / len(val_loader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc

# Add this import at the top of your file with other imports
from torch.cuda.amp import GradScaler, autocast

# Training Loop
best_val_acc = 0.0
patience = 2  # Number of epochs to wait before early stopping
no_improve = 0  # Counter for epochs without improvement
early_stop = False  # Flag for early stopping

# Initialize gradient scaler for mixed precision training
scaler = GradScaler()

for epoch in range(num_epochs):
    if early_stop:
        print("Early stopping triggered")
        break

    print(f"\nEpoch {epoch+1}/{num_epochs}")
    print("-" * 50)

    # Train with mixed precision
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(train_loader, desc="Training")
    for batch in progress_bar:
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        # Mixed precision forward pass
        with autocast():
            outputs = model(pixel_values)
            loss = criterion(outputs.logits, labels)

        # Scaled backward pass
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # Statistics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.logits.data, 1)
        total_predictions += labels.size(0)
        correct_predictions += (predicted == labels).sum().item()

        progress_bar.set_postfix({
            'Loss': f'{running_loss/len(progress_bar):.4f}',
            'Acc': f'{100*correct_predictions/total_predictions:.2f}%'
        })

    train_loss = running_loss / len(train_loader)
    train_acc = correct_predictions / total_predictions

    # Validate
    model.eval()
    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)

    # Update scheduler
    scheduler.step()

    # Save history
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)

    # Print epoch results
    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Check for improvement
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        no_improve = 0
        torch.save(model.state_dict(), 'best_vivit_deepfake_model.pth')
        print(f"New best model saved with validation accuracy: {val_acc:.4f}")
    else:
        no_improve += 1
        if no_improve >= patience:
            early_stop = True

    # Optional: Add learning rate logging
    current_lr = optimizer.param_groups[0]['lr']
    print(f"Current learning rate: {current_lr:.2e}")

print(f"\nTraining completed! Best validation accuracy: {best_val_acc:.4f}")

# Plot training history
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# Plot losses
ax1.plot(train_losses, label='Train Loss')
ax1.plot(val_losses, label='Validation Loss')
ax1.set_title('Training and Validation Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend()
ax1.grid(True)

# Plot accuracies
ax2.plot(train_accuracies, label='Train Accuracy')
ax2.plot(val_accuracies, label='Validation Accuracy')
ax2.set_title('Training and Validation Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()

# Evaluation on Test Set

# Load best model
model.load_state_dict(torch.load('best_vivit_deepfake_model.pth'))
model.eval()

# Evaluate on test set
test_predictions = []
test_labels_list = []

print("Evaluating on test set...")
with torch.no_grad():
    for batch in tqdm(test_loader, desc="Testing"):
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(pixel_values)
        _, predicted = torch.max(outputs.logits.data, 1)

        test_predictions.extend(predicted.cpu().numpy())
        test_labels_list.extend(labels.cpu().numpy())

# Calculate metrics
test_accuracy = accuracy_score(test_labels_list, test_predictions)
print(f"\nTest Accuracy: {test_accuracy:.4f}")

# Classification report
print("\nClassification Report:")
print(classification_report(test_labels_list, test_predictions,
                          target_names=['Real', 'Fake']))

# Plot confusion matrix
cm = confusion_matrix(test_labels_list, test_predictions)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Real', 'Fake'],
            yticklabels=['Real', 'Fake'])
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Print detailed metrics
tn, fp, fn, tp = cm.ravel()
precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1 = 2 * (precision * recall) / (precision + recall)

print(f"True Negatives: {tn}")
print(f"False Positives: {fp}")
print(f"False Negatives: {fn}")
print(f"True Positives: {tp}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# Save Final Model and Results

from google.colab import drive
if not os.path.exists('/content/gdrive/MyDrive'):
    drive.mount('/content/gdrive')
else:
    print("Google Drive already mounted.")

gdrive_base_path = '/content/gdrive/MyDrive/deepfake_detection_results'
os.makedirs(gdrive_base_path, exist_ok=True)
print(f"Saving to Google Drive path: {gdrive_base_path}")

# Define full paths for saving
model_save_path = os.path.join(gdrive_base_path, 'vivit_deepfake_final_model.pth')
results_save_path = os.path.join(gdrive_base_path, 'deepfake_detection_results.json')

# Save final model
torch.save({
    'model_state_dict': model.state_dict(),
    'config': model.config, # Ensure model.config exists and is serializable
    'test_accuracy': test_accuracy,
    'training_history': {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accuracies': train_accuracies,
        'val_accuracies': val_accuracies
    }
}, model_save_path)

# Save results to JSON
results = {
    'test_accuracy': float(test_accuracy),
    'precision': float(precision),
    'recall': float(recall),
    'f1_score': float(f1),
    'confusion_matrix': cm.tolist(), # Convert numpy array to list for JSON
    'num_epochs': num_epochs,
    'learning_rate': learning_rate,
    'batch_size': batch_size,
    'dataset_size': len(all_videos) # Assuming 'all_videos' holds your dataset
}

with open(results_save_path, 'w') as f:
    json.dump(results, f, indent=2)

print("Model and results saved successfully to Google Drive!")
print(f"Final test accuracy: {test_accuracy:.4f}")